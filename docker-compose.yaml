version: "3"
services:
  otel-collector:
    image: otel/opentelemetry-collector:latest
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    networks:
      - dev
    ports:
      - "4317:4317"     # OTLP gRPC receiver
      - "4318:4318"     # OTLP HTTP receiver
      - "8889:8889"     # Prometheus metrics exporter

    volumes:
      - ./tools/sre/opentelemetry/otel-collector-config.yaml:/etc/otel-collector-config.yaml

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    networks:
      - dev
    ports:
      - "9090:9090"     # Prometheus web interface
    volumes:
      - ./tools/sre/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - otel-collector

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    restart: unless-stopped
    ports:
      - "9093:9093"
    networks:
      - dev
    volumes:
      - "./tools/sre/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml"
      - alertmanager-data:/data
    command: --config.file=/etc/alertmanager/alertmanager.yml --log.level=debug

  jaeger:
    image: jaegertracing/all-in-one:1.60
    container_name: jaeger
    networks:
      - dev
    command: ["--collector.otlp.enabled=true", "--collector.otlp.grpc.host-port=4320", "--collector.otlp.http.host-port=4321"]
    environment:
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    ports:
      - "14268:14268"  # HTTP transport, applications or agents send trace data to this endpoint over HTTP
      - "14267:14267"  # gRPC transport, applications or agents send trace data to this endpoint over gRPC
      - "14399:14399"  # JAEGER_QUERY_PORT
      - "16686:16686"  # Jaeger UI
      - "14250:14250"  # Jaeger gRPC, used for gRPC communication within Jaeger's architecture
      - "5778:5778"    # Jaeger Admin
      - "44317:4320"   # Onboard Otel Collector gRPC port
      - "44318:4321"   # Onboard Otel Collector HTTP port
    depends_on:
      - otel-collector
      - prometheus


  # Elasticsearch service
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.7.0
    container_name: elasticsearch
    environment:
      - node.name=es01
      - cluster.name=logs-cluster
      - discovery.type=single-node
      - ELASTIC_PASSWORD=changeme
      - xpack.security.enabled=false
      # - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    deploy:
      resources:
        limits:
          memory: 512M
    ports:
      - "9200:9200"  # HTTP
      - "9300:9300"  # Transport
    networks:
      - dev
    volumes:
      - elastic-data:/usr/share/elasticsearch/data
    depends_on:
      - otel-collector

  logstash:
    image: docker.elastic.co/logstash/logstash:8.7.0
    container_name: logstash
    networks:
      - dev
    ports:
      - "5044:5045"    # Logstash Beats input
    deploy:
      resources:
        limits:
          memory: 512M
    # environment:
    #   - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
    # command: >
    #   sh -c "/usr/local/bin/docker-entrypoint -Xms256m -Xmx256m"
    volumes:
      - ./tools/sre/logstash/logstash_config.conf:/usr/share/logstash/config/logstash.yml
      - ./tools/sre/logstash/logstash_pipeline.conf/:/usr/share/logstash/pipeline/pipeline.conf  # Pipeline configurations
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.7.0
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    deploy:
      resources:
        limits:
          memory: 512M
    volumes:
      - kibana-data:/usr/share/kibana/data
    networks:
      - dev

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=grafana  # Set the admin password
    volumes:
      - grafana-data:/var/lib/grafana
      - ./tools/sre/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./tools/sre/grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - dev
    depends_on:
      - otel-collector
      - prometheus
      #- api

  # heimdall:
  #   image: linuxserver/heimdall:latest
  #   container_name: heimdall
  #   ports:
  #     - "8091:80"
  #   volumes:
  #     - heimdall-data:/config
  #   networks:
  #     - dev

  # api:
  #   build:
  #     context: .
  #     dockerfile: ./src/api/v1/aiden/Dockerfile
  #   image: aiden/api:latest  # Use the custom image name here
  #   container_name: aiden-api
  #   networks:
  #     - dev
  #   ports:
  #     - "8000:8000"
  #   depends_on:
  #     - otel-collector

   # MLFlow Tracking Server to track experiments for ML
  # mlflow:
  #   restart: always
  #   build:
  #     context: .
  #     dockerfile: ./tools/mlops/mlflow/Dockerfile
  #   image: mlflow/mlflow:latest  # Use the custom image name here
  #   container_name: mlflow
  #   depends_on:
  #     - postgres
  #     - minio
  #   ports:
  #     - "5000:5000"
  #   networks:
  #     - dev
  #   environment:
  #     - AWS_ACCESS_KEY_ID=definitelyinsecure
  #     - AWS_SECRET_ACCESS_KEY=notsosecretisitnow
  #     - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
  #     - MLFLOW_S3_IGNORE_TLS=true
  #   command: >
  #     mlflow server
  #     --backend-store-uri postgresql://postgres:postgres@postgres:5432/postgres
  #     --host 0.0.0.0
  #     --serve-artifacts
  #     --artifacts-destination minio://mlflow
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://mlflow:5000/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # postgres:
  #   restart: always
  #   image: postgres
  #   container_name: postgresql
  #   networks:
  #     - dev
  #   environment:
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=postgres
  #     - POSTGRES_DATABASE=mlflow
  #   volumes:
  #     - postgres-data:/var/lib/postgresql/data/
  #   healthcheck:
  #     test: ["CMD", "pg_isready", "-p", "5432", "-U", "postgres"]
  #     interval: 5s
  #     timeout: 5s
  #     retries: 3

  # S3 storage for MLFlow
  # minio:
  #   restart: always
  #   image: minio/minio:latest
  #   container_name: minio
  #   volumes:
  #     - minio-data:/data
  #   ports:
  #     - "9000:9000"
  #     - "9001:9001"
  #   networks:
  #     - dev
  #   environment:
  #     - MINIO_ROOT_USER=minioadmin
  #     - MINIO_ROOT_PASSWORD=minioadmin
  #     - MINIO_ADDRESS=:9000
  #     - MINIO_PORT=9000
  #     - MINIO_STORAGE_USE_HTTPS=False
  #     - MINIO_CONSOLE_ADDRESS=:9001
  #   command: server --console-address ":9001" /data
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
  #     interval: 30s
  #     timeout: 20s
  #     retries: 3

  # job_create_s3_bucket:
  #   container_name: job_create_bucket
  #   image: minio/mc
  #   depends_on:
  #     - minio
  #   networks:
  #     - dev
  #   entrypoint: >
  #     /bin/sh -c "
  #     /usr/bin/mc alias set s3minio http://minio:9000 minioadmin minioadmin;
  #     /usr/bin/mc mb s3minio/mlflow;
  #     /usr/bin/mc policy set public s3minio/mlflow;
  #     exit 0;
  #     "

  # job_create_s3_accesskey:
  #   container_name: job_create_accesskey
  #   image: minio/mc
  #   depends_on:
  #     - minio
  #   networks:
  #     - dev
  #   entrypoint: >
  #     /bin/sh -c "
  #     /usr/bin/mc alias set s3minio http://minio:9000 minioadmin minioadmin;
  #     /usr/bin/mc admin user svcacct add --access-key "definitelyinsecure" --secret-key "notsosecretisitnow" --name mlflow --description connection s3minio minioadmin;
  #     exit 0;
  #     "

networks:
  dev:

volumes:
  grafana-data:
  elastic-data:
  kibana-data:
  heimdall-data:
  alertmanager-data:
  minio-data:
  postgres-data: