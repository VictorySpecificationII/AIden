version: "3"
services:
      
  otel-collector:
    image: otel/opentelemetry-collector:latest
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    networks:
      - dev
    ports:
      - "4317:4317"     # OTLP gRPC receiver
      - "4318:4318"     # OTLP HTTP receiver
      - "8889:8889"     # Prometheus metrics exporter

    volumes:
      - ./tools/sre/opentelemetry/otel-collector-config.yaml:/etc/otel-collector-config.yaml


  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:4.1.1-4.0.4-ubuntu22.04
    container_name: dcgm-exporter
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "9400:9400"  # Prometheus will scrape metrics from here
    command: ["dcgm-exporter"]
    networks:
      - dev


  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    networks:
      - dev
    ports:
      - "9090:9090"     # Prometheus web interface
    volumes:
      - ./tools/sre/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    depends_on:
      - otel-collector
    labels:
      homepage.group: "Observability"
      homepage.name: "Prometheus"
      homepage.icon: "prometheus"
      homepage.href: "http://localhost:9090"

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    restart: unless-stopped
    ports:
      - "9093:9093"
    networks:
      - dev
    volumes:
      - "./tools/sre/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml"
      - alertmanager-data:/data
    command: --config.file=/etc/alertmanager/alertmanager.yml --log.level=debug

  jaeger:
    image: jaegertracing/all-in-one:1.60
    container_name: jaeger
    networks:
      - dev
    command: ["--collector.otlp.enabled=true", "--collector.otlp.grpc.host-port=4320", "--collector.otlp.http.host-port=4321"]
    environment:
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    ports:
      - "14268:14268"  # HTTP transport, applications or agents send trace data to this endpoint over HTTP
      - "14267:14267"  # gRPC transport, applications or agents send trace data to this endpoint over gRPC
      - "14399:14399"  # JAEGER_QUERY_PORT
      - "16686:16686"  # Jaeger UI
      - "14250:14250"  # Jaeger gRPC, used for gRPC communication within Jaeger's architecture
      - "5778:5778"    # Jaeger Admin
      - "44317:4320"   # Onboard Otel Collector gRPC port
      - "44318:4321"   # Onboard Otel Collector HTTP port
    depends_on:
      - otel-collector
      - prometheus
    labels:
      homepage.group: "Observability"
      homepage.name: "Jaeger"
      homepage.icon: "jaeger"
      homepage.href: "http://localhost:16686"


  # Elasticsearch service
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.7.0
    container_name: elasticsearch
    environment:
      - node.name=es01
      - cluster.name=logs-cluster
      - discovery.type=single-node
      - ELASTIC_PASSWORD=changeme
      - xpack.security.enabled=false
      # - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    deploy:
      resources:
        limits:
          memory: 512M
    ports:
      - "9200:9200"  # HTTP
      - "9300:9300"  # Transport
    networks:
      - dev
    volumes:
      - elastic-data:/usr/share/elasticsearch/data
    depends_on:
      - otel-collector

  logstash:
    image: docker.elastic.co/logstash/logstash:8.7.0
    container_name: logstash
    networks:
      - dev
    ports:
      - "5044:5045"    # Logstash Beats input
    deploy:
      resources:
        limits:
          memory: 512M
    # environment:
    #   - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
    # command: >
    #   sh -c "/usr/local/bin/docker-entrypoint -Xms256m -Xmx256m"
    volumes:
      - ./tools/sre/logstash/logstash_config.conf:/usr/share/logstash/config/logstash.yml
      - ./tools/sre/logstash/logstash_pipeline.conf/:/usr/share/logstash/pipeline/pipeline.conf  # Pipeline configurations
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.7.0
    container_name: kibana
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    deploy:
      resources:
        limits:
          memory: 512M
    volumes:
      - kibana-data:/usr/share/kibana/data
    networks:
      - dev
    labels:
      homepage.group: "Observability"
      homepage.name: "Kibana"
      homepage.icon: "kibana"
      homepage.href: "http://localhost:5601"

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=grafana  # Set the admin password
    volumes:
      - grafana-data:/var/lib/grafana
      - ./tools/sre/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./tools/sre/grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - dev
    depends_on:
      - otel-collector
      - prometheus
    labels:
      homepage.group: "Observability"
      homepage.name: "Grafana"
      homepage.icon: "grafana"
      homepage.href: "http://localhost:3000"

  # heimdall:
  #   image: linuxserver/heimdall:latest
  #   container_name: heimdall
  #   ports:
  #     - "8091:80"
  #   volumes:
  #     - heimdall-data:/config
  #   networks:
  #     - dev

  homepage:
    image: ghcr.io/gethomepage/homepage:latest
    container_name: homepage
    ports:
      - 8092:3000
    volumes:
      - ./tools/sre/homepage:/app/config # Make sure your local config directory exists
      - /var/run/docker.sock:/var/run/docker.sock # (optional) For docker integrations
      - ./tools/sre/homepage/background:/app/public/images
    environment:
      HOMEPAGE_ALLOWED_HOSTS: "*" # required, may need port. See gethomepage.dev/installation/#homepage_allowed_hosts
    networks:
      - dev

  aiden:
    build:
      context: ./src/aiden
      dockerfile: Dockerfile
    image: intellectualplayspace/aiden
    container_name: AIden
    ports:
      - 8000:8000
    networks:
      - dev
    labels:
      homepage.group: "Co-Pilot"
      homepage.name: "Aiden"
      homepage.icon: "docker"
      homepage.href: "http://localhost:8000"

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant-data:/qdrant/storage
    restart: always
    networks:
      - dev
    labels:
      homepage.group: "Storage"
      homepage.name: "Qdrant"
      homepage.icon: "si-knowledgebase"
      homepage.href: "http://localhost:6333/dashboard"


  ollama:
    volumes:
      - ollama-data-gpu:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: docker.io/ollama/ollama:latest
    ports:
      - 11434:11434
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    networks:
      - dev
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 
              capabilities: [gpu]
    labels:
      homepage.group: "Machine Learning"
      homepage.name: "Ollama"
      homepage.icon: "si-ollama"
      homepage.href: "http://localhost:11434"

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:cuda
    container_name: ollama-webui
    runtime: nvidia
    volumes:
      - ollama-data-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8080:8080
    environment:
      - OLLAMA_BASE_URLS=http://host.docker.internal:11434
      - ENV=dev
      - WEBUI_AUTH=False
      - WEBUI_NAME=valiantlynx AI
      - WEBUI_URL=http://localhost:8080
      - WEBUI_SECRET_KEY=t0p-s3cr3t
      - RAG_EMBEDDING_MODEL_DEVICE_TYPE=cuda
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - dev
    labels:
      homepage.group: "Machine Learning"
      homepage.name: "OpenWebUI"
      homepage.icon: "si-openai"
      homepage.href: "http://localhost:8080"

  # MLFlow Tracking Server to track experiments for ML
  mlflow:
    restart: always
    build:
      context: .
      dockerfile: ./tools/mlops/mlflow/Dockerfile
    image: intellectualplayspace/mlflow  # Use the custom image name here
    container_name: mlflow
    depends_on:
      - postgres
      - minio
    ports:
      - "5000:5000"
    networks:
      - dev
    environment:
      - AWS_ACCESS_KEY_ID=definitelyinsecure
      - AWS_SECRET_ACCESS_KEY=notsosecretisitnow
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - MLFLOW_S3_IGNORE_TLS=true
    command: >
      mlflow server
      --backend-store-uri postgresql://postgres:postgres@postgres:5432/postgres
      --host 0.0.0.0
      --serve-artifacts
      --artifacts-destination s3://mlflow
    healthcheck:
      test: ["CMD", "curl", "-f", "http://mlflow:5000/"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      homepage.group: "Machine Learning"
      homepage.name: "MLFlow"
      homepage.icon: "si-mlflow"
      homepage.href: "http://localhost:5000"


  postgres:
    restart: always
    image: postgres
    container_name: postgresql
    networks:
      - dev
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DATABASE=mlflow
    volumes:
      - postgres-data:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD", "pg_isready", "-p", "5432", "-U", "postgres"]
      interval: 5s
      timeout: 5s
      retries: 3
    labels:
      homepage.group: "Storage"
      homepage.name: "PostgreSQL"
      homepage.icon: "postgres"
      homepage.href: "http://localhost:5432"

  # S3 storage for MLFlow
  minio:
    restart: always
    image: minio/minio:RELEASE.2025-04-22T22-12-26Z #LAST VERSION BEFORE THEY RUINED MINIO, DON'T CHANGE
    container_name: minio
    volumes:
      - minio-data:/data
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - dev
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - MINIO_ADDRESS=:9000
      - MINIO_PORT=9000
      - MINIO_STORAGE_USE_HTTPS=False
      - MINIO_CONSOLE_ADDRESS=:9001
      - MINIO_PROMETHEUS_AUTH_TYPE=public
    command: server --console-address ":9001" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    labels:
      homepage.group: "Storage"
      homepage.name: "MinIO"
      homepage.icon: "minio"
      homepage.href: "http://localhost:9001"


  minio_init:
    container_name: minio_init_container
    image: minio/mc
    depends_on:
      - minio
    networks:
      - dev
    entrypoint: >
      /bin/sh -c "
      sleep 30s;
      /usr/bin/mc alias set s3minio http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc admin user svcacct add --access-key "definitelyinsecure" --secret-key "notsosecretisitnow" --name mlflow --description connection s3minio minioadmin;
      /usr/bin/mc mb s3minio/mlflow;
      /usr/bin/mc policy set public s3minio/mlflow;
      exit 0;
      "

networks:
  dev:

volumes:
  grafana-data:
  elastic-data:
  kibana-data:
  # heimdall-data:
  alertmanager-data:
  minio-data:
  postgres-data:
  ollama-data-gpu:
  ollama-data-webui:
  qdrant-data: